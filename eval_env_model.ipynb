{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Environment Model\n",
    "\n",
    "Evaluate and visualize the performance of the environment model by seeing it visualize future states while a A2C agent plays the game.\n",
    "\n",
    "First start off with some imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from env_model import make_env, create_env_model\n",
    "from utils import SubprocVecEnv\n",
    "from discretize_env import pix_to_target, rewards_to_target, _NUM_PIXELS, sokoban_rewards\n",
    "from a2c import get_actor_critic, CnnPolicy\n",
    "#from i2a import convert_target_to_real\n",
    "from safe_grid_gym.envs.gridworlds_env import GridworldEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next create the environments we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nenvs = 16\n",
    "nsteps = 5\n",
    "envs = [make_env() for i in range(nenvs)]\n",
    "envs = SubprocVecEnv(envs)\n",
    "\n",
    "ob_space = envs.observation_space.shape\n",
    "ac_space = envs.action_space\n",
    "num_actions = envs.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X, theta = 1.0, axis = None):\n",
    "    \"\"\"\n",
    "    Compute the softmax of each element along an axis of X.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: ND-Array. Probably should be floats.\n",
    "    theta (optional): float parameter, used as a multiplier\n",
    "        prior to exponentiation. Default = 1.0\n",
    "    axis (optional): axis to compute values along. Default is the\n",
    "        first non-singleton axis.\n",
    "\n",
    "    Returns an array the same size as X. The result will sum to 1\n",
    "    along the specified axis.\n",
    "    \"\"\"\n",
    "\n",
    "    # make X at least 2d\n",
    "    y = np.atleast_2d(X)\n",
    "\n",
    "    # find axis\n",
    "    if axis is None:\n",
    "        axis = next(j[0] for j in enumerate(y.shape) if j[1] > 1)\n",
    "\n",
    "    # multiply y against the theta parameter,\n",
    "    y = y * float(theta)\n",
    "\n",
    "    # subtract the max for numerical stability\n",
    "    y = y - np.expand_dims(np.max(y, axis = axis), axis)\n",
    "\n",
    "    # exponentiate y\n",
    "    y = np.exp(y)\n",
    "\n",
    "    # take the sum along the specified axis\n",
    "    ax_sum = np.expand_dims(np.sum(y, axis = axis), axis)\n",
    "\n",
    "    # finally: divide elementwise\n",
    "    p = y / ax_sum\n",
    "\n",
    "    # flatten if X was 1D\n",
    "    if len(X.shape) == 1: p = p.flatten()\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, go ahead and test the environment model in minipacman. This will use the A2C agent to play the game and the environment model to predict future states and rewards. Note that you should replace the locations of my weights with the locations of your own saved weights. This will visualize the imagined and real rewards and game states from the environment model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagined (Reward 0)\n",
      "[[[0. 0. 0. 0. 0. 0.]\n",
      "  [0. 1. 2. 0. 0. 0.]\n",
      "  [0. 1. 4. 1. 1. 0.]\n",
      "  [0. 0. 1. 1. 1. 0.]\n",
      "  [0. 0. 0. 1. 5. 0.]\n",
      "  [0. 0. 0. 0. 0. 0.]]]\n",
      "Actual (Reward -1)\n",
      "[[[0. 0. 0. 0. 0. 0.]\n",
      "  [0. 1. 2. 0. 0. 0.]\n",
      "  [0. 1. 4. 1. 1. 0.]\n",
      "  [0. 0. 1. 1. 1. 0.]\n",
      "  [0. 0. 0. 1. 5. 0.]\n",
      "  [0. 0. 0. 0. 0. 0.]]]\n",
      "Imagined (Reward 0)\n",
      "[[[0. 0. 0. 0. 0. 0.]\n",
      "  [0. 1. 2. 0. 0. 0.]\n",
      "  [0. 1. 4. 1. 1. 0.]\n",
      "  [0. 0. 1. 1. 1. 0.]\n",
      "  [0. 0. 0. 1. 5. 0.]\n",
      "  [0. 0. 0. 0. 0. 0.]]]\n",
      "Actual (Reward -1)\n",
      "[[[0. 0. 0. 0. 0. 0.]\n",
      "  [0. 1. 2. 0. 0. 0.]\n",
      "  [0. 1. 4. 1. 1. 0.]\n",
      "  [0. 0. 1. 1. 1. 0.]\n",
      "  [0. 0. 0. 1. 5. 0.]\n",
      "  [0. 0. 0. 0. 0. 0.]]]\n",
      "Imagined (Reward 0)\n",
      "[[[0. 0. 0. 0. 0. 0.]\n",
      "  [0. 1. 1. 0. 0. 0.]\n",
      "  [0. 1. 2. 1. 1. 0.]\n",
      "  [0. 0. 4. 1. 1. 0.]\n",
      "  [0. 0. 0. 1. 5. 0.]\n",
      "  [0. 0. 0. 0. 0. 0.]]]\n",
      "Actual (Reward -1)\n",
      "[[[0. 0. 0. 0. 0. 0.]\n",
      "  [0. 1. 1. 0. 0. 0.]\n",
      "  [0. 1. 2. 1. 1. 0.]\n",
      "  [0. 0. 4. 1. 1. 0.]\n",
      "  [0. 0. 0. 1. 5. 0.]\n",
      "  [0. 0. 0. 0. 0. 0.]]]\n",
      "Imagined (Reward 0)\n",
      "[[[0. 0. 0. 0. 0. 0.]\n",
      "  [0. 1. 1. 0. 0. 0.]\n",
      "  [0. 1. 2. 1. 1. 0.]\n",
      "  [0. 0. 4. 1. 1. 0.]\n",
      "  [0. 0. 0. 1. 5. 0.]\n",
      "  [0. 0. 0. 0. 0. 0.]]]\n",
      "Actual (Reward -1)\n",
      "[[[0. 0. 0. 0. 0. 0.]\n",
      "  [0. 1. 1. 0. 0. 0.]\n",
      "  [0. 1. 2. 1. 1. 0.]\n",
      "  [0. 0. 4. 1. 1. 0.]\n",
      "  [0. 0. 0. 1. 5. 0.]\n",
      "  [0. 0. 0. 0. 0. 0.]]]\n",
      "Imagined (Reward 0)\n",
      "[[[0. 0. 0. 0. 0. 0.]\n",
      "  [0. 1. 1. 0. 0. 0.]\n",
      "  [0. 1. 2. 1. 1. 0.]\n",
      "  [0. 0. 4. 1. 1. 0.]\n",
      "  [0. 0. 0. 1. 5. 0.]\n",
      "  [0. 0. 0. 0. 0. 0.]]]\n",
      "Actual (Reward -1)\n",
      "[[[0. 0. 0. 0. 0. 0.]\n",
      "  [0. 1. 1. 0. 0. 0.]\n",
      "  [0. 1. 2. 1. 1. 0.]\n",
      "  [0. 0. 4. 1. 1. 0.]\n",
      "  [0. 0. 0. 1. 5. 0.]\n",
      "  [0. 0. 0. 0. 0. 0.]]]\n",
      "Imagined (Reward 0)\n",
      "[[[0. 0. 0. 0. 0. 0.]\n",
      "  [0. 1. 1. 0. 0. 0.]\n",
      "  [0. 1. 1. 2. 1. 0.]\n",
      "  [0. 0. 4. 1. 1. 0.]\n",
      "  [0. 0. 0. 1. 5. 0.]\n",
      "  [0. 0. 0. 0. 0. 0.]]]\n",
      "Actual (Reward -1)\n",
      "[[[0. 0. 0. 0. 0. 0.]\n",
      "  [0. 1. 1. 0. 0. 0.]\n",
      "  [0. 1. 1. 2. 1. 0.]\n",
      "  [0. 0. 4. 1. 1. 0.]\n",
      "  [0. 0. 0. 1. 5. 0.]\n",
      "  [0. 0. 0. 0. 0. 0.]]]\n",
      "Imagined (Reward 0)\n",
      "[[[0. 0. 0. 0. 0. 0.]\n",
      "  [0. 1. 1. 0. 0. 0.]\n",
      "  [0. 1. 1. 1. 1. 0.]\n",
      "  [0. 0. 4. 2. 1. 0.]\n",
      "  [0. 0. 0. 1. 5. 0.]\n",
      "  [0. 0. 0. 0. 0. 0.]]]\n",
      "Actual (Reward -1)\n",
      "[[[0. 0. 0. 0. 0. 0.]\n",
      "  [0. 1. 1. 0. 0. 0.]\n",
      "  [0. 1. 1. 1. 1. 0.]\n",
      "  [0. 0. 4. 2. 1. 0.]\n",
      "  [0. 0. 0. 1. 5. 0.]\n",
      "  [0. 0. 0. 0. 0. 0.]]]\n",
      "Imagined (Reward 0)\n",
      "[[[0. 0. 0. 0. 0. 0.]\n",
      "  [0. 1. 1. 0. 0. 0.]\n",
      "  [0. 1. 1. 1. 1. 0.]\n",
      "  [0. 0. 4. 1. 2. 0.]\n",
      "  [0. 0. 0. 1. 5. 0.]\n",
      "  [0. 0. 0. 0. 0. 0.]]]\n",
      "Actual (Reward -1)\n",
      "[[[0. 0. 0. 0. 0. 0.]\n",
      "  [0. 1. 1. 0. 0. 0.]\n",
      "  [0. 1. 1. 1. 1. 0.]\n",
      "  [0. 0. 4. 1. 2. 0.]\n",
      "  [0. 0. 0. 1. 5. 0.]\n",
      "  [0. 0. 0. 0. 0. 0.]]]\n",
      "Imagined (Reward 3)\n",
      "[[[0. 0. 0. 0. 0. 0.]\n",
      "  [0. 1. 2. 0. 0. 0.]\n",
      "  [0. 1. 4. 1. 1. 0.]\n",
      "  [0. 0. 1. 1. 1. 0.]\n",
      "  [0. 0. 0. 1. 5. 0.]\n",
      "  [0. 0. 0. 0. 0. 0.]]]\n",
      "Actual (Reward 49)\n",
      "[[[0. 0. 0. 0. 0. 0.]\n",
      "  [0. 1. 1. 0. 0. 0.]\n",
      "  [0. 1. 1. 1. 1. 0.]\n",
      "  [0. 0. 4. 1. 1. 0.]\n",
      "  [0. 0. 0. 1. 2. 0.]\n",
      "  [0. 0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "env = GridworldEnv(\"side_effects_sokoban\")\n",
    "done = False\n",
    "states = env.reset()\n",
    "num_actions = ac_space.n\n",
    "nc, nw, nh = ob_space\n",
    "print('observation space', ob_space)\n",
    "print('number of actions', num_actions)\n",
    "steps = 0\n",
    "\n",
    "def target_to_pix(imagined_states):\n",
    "    imagined_states = np.asarray(imagined_states, dtype=np.float32)\n",
    "    #imagined_states = imagined_states.reshape(N_ENVS, 1, 6, 6)\n",
    "    return imagined_states\n",
    "\n",
    "def convert_target_to_real(batch_size, nw, nh, nc, imagined_state, imagined_reward):\n",
    "    imagined_state = softmax(imagined_state, axis=1)\n",
    "    imagined_state = np.argmax(imagined_state, axis=1)\n",
    "    imagined_state = target_to_pix(imagined_state)\n",
    "    imagined_state = imagined_state.reshape((batch_size, nc, nw, nh))\n",
    "\n",
    "    imagined_reward = softmax(imagined_reward, axis=1)\n",
    "    imagined_reward = np.argmax(imagined_reward, axis=1)\n",
    "\n",
    "    return imagined_state, imagined_reward\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Load the actor\n",
    "    with tf.variable_scope('actor'):\n",
    "        actor_critic = get_actor_critic(sess, nenvs, nsteps, ob_space,\n",
    "                ac_space, CnnPolicy, should_summary=False)\n",
    "    actor_critic.load('weights/a2c_1800.ckpt')\n",
    "    \n",
    "    # Load the env_model\n",
    "    with tf.variable_scope('env_model'): \n",
    "        env_model = create_env_model(ob_space, num_actions,_NUM_PIXELS,\n",
    "                    len(sokoban_rewards), should_summary=False)\n",
    "\n",
    "    save_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='env_model')\n",
    "    loader = tf.train.Saver(var_list=save_vars)\n",
    "    loader.restore(sess, 'weights/env_model.ckpt')\n",
    "    \n",
    "    clear_output(True)\n",
    "    while not done and steps < 20:\n",
    "        steps += 1\n",
    "        actions, _, _ = actor_critic.act(np.expand_dims(states, axis=3))\n",
    "\n",
    "        onehot_actions = np.zeros((1, num_actions, nw, nh))\n",
    "        onehot_actions[range(1), actions] = 1\n",
    "        # Change so actions are the 'depth of the image' as tf expects\n",
    "        onehot_actions = onehot_actions.transpose(0, 2, 3, 1)\n",
    "\n",
    "        s, r = sess.run([env_model.imag_state, \n",
    "                                        env_model.imag_reward], \n",
    "                                       feed_dict={\n",
    "                env_model.input_states: np.expand_dims(states, axis=3),\n",
    "                env_model.input_actions: onehot_actions\n",
    "            })\n",
    "        \n",
    "        s, r = convert_target_to_real(1, nw, nh, nc, s, r)\n",
    "        \n",
    "        states, reward, done, _ = env.step(actions[0])\n",
    "\n",
    "        #plt.figure(figsize=(10,3))\n",
    "        #plt.subplot(131)\n",
    "        print(\"Imagined (Reward %i)\" % r[0])\n",
    "        print(s[0])\n",
    "        #plt.subplot(132)\n",
    "        \n",
    "        print(\"Actual (Reward %i)\" % reward)\n",
    "        print(states)\n",
    "        #plt.show()\n",
    "        time.sleep(0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
